# End-to-end pipeline (Parallelized Version for quicker processing of large datasets)
# Extract infraspecific taxon names from World Flora Online and GRIN-Global
#
# Notes:
# - This version is optimized for performance using parallel processing.
# - It includes thread-safe caching and corrected logic for data handling.
# - Fallback scan of combined resolver results to reduce OUTLINK fallbacks
# - Produces one Excel file: infraspecifics_verified.xlsx

# ---- dependencies ----
required_pkgs <- c("httr", "jsonlite", "stringr", "dplyr", "tidyr", "rvest", "xml2", "readr", "writexl", "tibble", "future", "furrr", "flock", "data.table")
for (p in required_pkgs) if (!requireNamespace(p, quietly = TRUE)) install.packages(p)
library(httr)
library(jsonlite)
library(stringr)
library(dplyr)
library(tidyr)
library(rvest)
library(xml2)
library(readr)
library(writexl)
library(tibble)
library(future)
library(furrr)
library(flock)
library(data.table)

# ---- source helper modules (adjust paths if needed) ----
# Ensure these helper scripts are in a 'Functions/' and 'Tools/' directory
# or adjust the paths accordingly.
source('Functions/Query_taxa_resolver.R')
source('Functions/Process_taxa_resolver_results.R')
source('Functions/parse_wfo_json_infraspecifics.R')
source('Functions/fetch_outlink_infraspecifics.R')
source('Tools/clean_outlink_tokens.R')

# ---- configuration ----
# Read in file with full list of species
plant_list <- read_csv("my_species_list.csv")  # example file read in

# extract species list
species_list <- unique(trimws(na.omit(plant_list$species_name_field))) # example spcies field name extract

sources <- list(WFO = '196', GRIN = '6') # adjust as needed
query_delay <- 0.12   # adjust as needed
outlink_delay <- 0.18  # adjust as needed

# ---- parallel setup ----
num_workers <- 2         # adjust as needed
plan(multisession, workers = num_workers)

# ---- provenance cache (deterministic path and lock) ----
prov_cache_file <- file.path(getwd(), "verifier_provenance_cache.rds")
prov_cache_lock <- file.path(getwd(), "prov_cache.lock")

# initialize prov_cache environment (main process)
prov_cache <- new.env(parent = emptyenv())
if (file.exists(prov_cache_file)) {
  saved <- tryCatch(readRDS(prov_cache_file), error = function(e) NULL)
  if (is.list(saved)) list2env(saved, envir = prov_cache)
}

# ---- small helpers ----
`%||%` <- function(a, b) if (is.null(a)) b else a
na_if_null <- function(x) if (is.null(x)) NA_character_ else x
trim_s <- function(x) trimws(as.character(x))

# canonicalize for comparison
canonical_compare <- function(x) {
  if (is.null(x) || is.na(x)) return(NA_character_)
  y <- tolower(as.character(x))
  y <- gsub("[[:punct:]]", " ", y)
  y <- gsub("\\s+", " ", trimws(y))
  y
}

# clean binomial (strip parentheses/authors)
clean_binomial <- function(sp) {
  s <- gsub("\\(.*?\\)", "", sp)
  s <- gsub(",.*$", "", s)
  s <- gsub("\\s+", " ", trimws(s))
  toks <- strsplit(s, "\\s+")[[1]]
  if (length(toks) >= 2) paste(toks[1], toks[2]) else sp
}

expand_queries_for_species <- function(species) {
  toks <- str_split(str_squish(species), "\\s+")[[1]]
  if (length(toks) < 2) return(character(0))
  genus <- toks[1]
  rank_prefixes <- c("var.", "variety", "subsp.", "subspecies", "f.", "forma", "subvar.", "subvariety", "cv.", "cultivar")
  unique(c(genus, paste(toks[1], toks[2]), paste(paste(toks[1], toks[2]), rank_prefixes), paste0(paste(toks[1], toks[2]), " ")))
}

canonicalize_infra <- function(name) {
  if (is.na(name) || !nzchar(name)) return(NA_character_)
  nm <- str_squish(as.character(name))
  toks_raw <- strsplit(nm, "\\s+")[[1]]
  toks_clean <- tolower(gsub("[[:punct:]]", "", toks_raw))
  if (length(toks_clean) < 3) return(NA_character_)
  rank_tokens <- c("subsp","subspecies","subvar","var","variety","forma","form","f","cultivar","cv","race")
  rank_idx <- which(toks_clean %in% rank_tokens)
  if (length(rank_idx) >= 1) {
    idx <- rank_idx[1]
    if (length(toks_clean) >= idx + 1) {
      return(paste(toks_clean[1], toks_clean[2], toks_clean[idx], toks_clean[idx + 1], sep = " "))
    }
  }
  third <- toks_raw[3]
  if (grepl("^[a-z]", third)) return(paste(toks_clean[1], toks_clean[2], toks_clean[3], sep = " "))
  NA_character_
}

infra_syntax_ok <- function(name, species_binomial) {
  if (is.null(name) || !nzchar(name)) return(FALSE)
  genus <- str_split(str_squish(species_binomial), "\\s+")[[1]][1]
  pattern <- paste0("^\\s*(", genus, "|", tolower(genus), ")\\s+[a-z\\-]+(?:\\s+(?:subsp\\.?|subspecies|subvar\\.?|var\\.?|variety|forma|f\\.?|form\\b|cv\\.?|cultivar|race)\\s+[a-z\\-]+|\\s+[a-z\\-]+)")
  if (!str_detect(name, regex(pattern))) return(FALSE)
  toks <- str_split(str_squish(name), "\\s+")[[1]]
  if (length(toks) < 3) return(FALSE)
  third <- toks[3]
  noise_words <- c("and","or","of","in","the","using","application","implications","exploitable","genotypes","estimated")
  if (tolower(third) %in% noise_words) return(FALSE)
  if (!grepl("^[a-z\\-]+$", third)) {
    if (!grepl("^(subsp\\.?|subspecies|subvar\\.?|var\\.?|variety|forma|f\\.?|form\\b|cv\\.?|cultivar|race)$", third, perl = TRUE)) {
      return(FALSE)
    }
  }
  TRUE
}

# ---- network wrappers ----
query_taxa_resolver_local <- function(taxa, sources_ids = c('196','6'), timeout_sec = 30) {
  if (!is.character(taxa) || length(taxa) != 1) return(NULL)
  taxa_encoded <- URLencode(taxa, reserved = TRUE)
  URL <- paste0('https://verifier.globalnames.org/api/v1/verifications/', taxa_encoded,
                '?data_sources=', paste(sources_ids, collapse = "|"),
                '&all_matches=true&capitalize=true&species_group=true',
                '&fuzzy_uninomial=false&stats=false&main_taxon_threshold=0.8')
  res <- try(GET(URL, user_agent("R (reverify)"), timeout(timeout_sec)), silent = TRUE)
  if (inherits(res, "try-error") || status_code(res) != 200) return(NULL)
  txt <- content(res, as = "text", encoding = "UTF-8")
  parsed <- try(fromJSON(txt, simplifyVector = FALSE), silent = TRUE)
  if (inherits(parsed, "try-error")) return(NULL)
  parsed
}

# ---- provenance helpers (main process only) ----
# More permissive source detection
query_source_for_name <- function(name, source_id, timeout_sec = 30) {
  cache_key <- paste0("src:", source_id, "|", name)
  if (exists(cache_key, envir = prov_cache, inherits = FALSE)) {
    return(get(cache_key, envir = prov_cache))
  }
  
  res <- tryCatch(query_taxa_resolver(name, sources = c(as.character(source_id)), verbose = FALSE, timeout_sec = timeout_sec),
                  error = function(e) NULL)
  if (is.null(res)) res <- tryCatch(query_taxa_resolver_local(name, sources_ids = c(as.character(source_id))), error = function(e) NULL)
  
  out <- list(ok = FALSE, matched = NA_character_, matchType = NA_character_, status = NA_character_)
  accepted_match_types <- c("exact", "exactspeciesgroup", "exactname", "partialfuzzy", "partial", "infra", "infraspecific")
  
  cand_can <- canonical_compare(name)
  
  if (!is.null(res) && is.list(res) && !is.null(res$names)) {
    for (nm in res$names) {
      results_list <- nm$results %||% list(nm)
      for (r in results_list) {
        dsid <- as.character(r$dataSourceId %||% r$data_source_id %||% NA)
        match_type <- tolower(as.character(r$matchType %||% r$match_type %||% ""))
        matched_nm <- trimws(r$matchedName %||% r$currentName %||% nm$name)
        matched_can <- canonical_compare(matched_nm)
        
        # direct dsid + accepted match type
        if (!is.na(dsid) && dsid == as.character(source_id) && match_type %in% accepted_match_types) {
          out$ok <- TRUE
          out$matched <- matched_nm
          out$matchType <- match_type
          out$status <- r$status %||% NA_character_
          break
        }
        
        # dsid matches but different match type: accept if canonical contains candidate
        if (!is.na(dsid) && dsid == as.character(source_id)) {
          if (!is.na(matched_can) && !is.na(cand_can) && grepl(cand_can, matched_can, fixed = TRUE)) {
            out$ok <- TRUE
            out$matched <- matched_nm
            out$matchType <- match_type
            out$status <- r$status %||% NA_character_
            break
          }
        }
        
        # fallback: no dsid but canonical match
        if (is.na(dsid) && !is.na(matched_can) && !is.na(cand_can) && grepl(cand_can, matched_can, fixed = TRUE)) {
          out$ok <- TRUE
          out$matched <- matched_nm
          out$matchType <- match_type
          out$status <- r$status %||% NA_character_
          break
        }
      }
      if (isTRUE(out$ok)) break
    }
  }
  
  lock <- flock::lock(prov_cache_lock)
  assign(cache_key, out, envir = prov_cache)
  saveRDS(as.list(prov_cache), prov_cache_file)
  flock::unlock(lock)
  
  out
}

# composite provenance determination (caches result)
determine_source <- function(candidate) {
  key_master <- paste0("prov|", candidate)
  if (exists(key_master, envir = prov_cache, inherits = FALSE)) return(get(key_master, envir = prov_cache))
  
  wfo <- query_source_for_name(candidate, sources$WFO)
  Sys.sleep(query_delay)
  grin <- query_source_for_name(candidate, sources$GRIN)
  
  src <- "NONE"
  if (isTRUE(wfo$ok) && isTRUE(grin$ok)) src <- "BOTH"
  else if (isTRUE(wfo$ok)) src <- "WFO"
  else if (isTRUE(grin$ok)) src <- "GRIN"
  else src <- "OUTLINK"
  
  res <- list(source = src, wfo = wfo, grin = grin)
  lock <- flock::lock(prov_cache_lock)
  assign(key_master, res, envir = prov_cache)
  saveRDS(as.list(prov_cache), prov_cache_file)
  flock::unlock(lock)
  res
}

# fallback scan across combined response for dsid evidence
fallback_check_candidate_sources <- function(candidate) {
  res <- query_taxa_resolver_local(candidate, sources_ids = c(sources$WFO, sources$GRIN))
  if (is.null(res) || is.null(res$names)) return("OUTLINK")
  cand_can <- canonical_compare(candidate)
  found_wfo <- FALSE; found_grin <- FALSE
  for (nm in res$names) {
    for (r in nm$results %||% list(nm)) {
      dsid <- as.character(r$dataSourceId %||% r$data_source_id %||% NA)
      matched_nm <- trimws(r$matchedName %||% r$currentName %||% nm$name)
      matched_can <- canonical_compare(matched_nm)
      if (!is.na(dsid) && !is.na(matched_can) && !is.na(cand_can) && grepl(cand_can, matched_can, fixed = TRUE)) {
        if (dsid == as.character(sources$WFO)) found_wfo <- TRUE
        if (dsid == as.character(sources$GRIN)) found_grin <- TRUE
      }
    }
  }
  if (found_wfo && found_grin) return("BOTH")
  if (found_wfo) return("WFO")
  if (found_grin) return("GRIN")
  "OUTLINK"
}

# ---- Stage A: parallel candidate gathering (workers only) ----
process_species_parallel <- function(sp) {
  tryCatch({
    sp_clean <- clean_binomial(sp)
    queries <- expand_queries_for_species(sp_clean)
    raw_responses <- list()
    combined_matches_list <- list()
    
    for (q in queries) {
      for (src_label in names(sources)) {
        Sys.sleep(query_delay)
        src_id <- sources[[src_label]]
        res <- tryCatch(query_taxa_resolver(q, sources = c(src_id), verbose = FALSE), error = function(e) NULL)
        if (is.null(res)) res <- tryCatch(query_taxa_resolver_local(q, sources_ids = c(src_id)), error = function(e) NULL)
        raw_responses[[paste(q, src_label, sep = "|||")]] <- res
        
        df_matches <- tryCatch({
          extract_infraspecifics_single(res, input_species = sp, source_name = src_label,
                                        accepted_only = FALSE, include_synonyms = TRUE, verbose = FALSE)
        }, error = function(e) tibble())
        
        if (nrow(df_matches) > 0) {
          combined_matches_list[[length(combined_matches_list) + 1]] <- df_matches
        }
      }
    }
    
    outlinks <- unique(na.omit(unlist(lapply(raw_responses, function(x) {
      if (is.null(x) || is.null(x$names)) return(NULL)
      ols <- c()
      for (n in x$names) {
        if (!is.null(n$results)) {
          for (r in n$results) if (!is.null(r$outlink) && nzchar(r$outlink)) ols <- c(ols, r$outlink)
        }
        if (!is.null(n$outlink) && nzchar(n$outlink)) ols <- c(ols, n$outlink)
      }
      ols
    }))))
    
    parsed_from_outlinks <- list()
    if (length(outlinks) > 0) {
      for (ol in outlinks) {
        Sys.sleep(outlink_delay)
        parsed <- tryCatch(fetch_outlink_infraspecifics(ol, species_binomial = sp, verbose = FALSE), error = function(e) NULL)
        if (!is.null(parsed)) parsed_from_outlinks[[ol]] <- parsed
      }
    }
    
    wfo_included <- unique(unlist(lapply(parsed_from_outlinks, function(x) x$included_varieties)))
    wfo_syns     <- unique(unlist(lapply(parsed_from_outlinks, function(x) x$synonyms)))
    page_candidates <- unique(unlist(lapply(parsed_from_outlinks, function(x) x$page_candidates)))
    
    # Build safe combined_matches if none found
    if (length(combined_matches_list) > 0) {
      combined_matches <- bind_rows(combined_matches_list) %>% distinct(input_species, source, candidate_name, .keep_all = TRUE)
      # Normalise source to character
      combined_matches$source_char <- toupper(as.character(combined_matches$source))
    } else {
      combined_matches <- tibble(input_species = character(), source = character(), candidate_name = character(), source_char = character())
    }
    
    grin_cands <- combined_matches %>% filter(source_char %in% c("GRIN", "6")) %>% pull(candidate_name) %>% unique()
    wfo_cands  <- combined_matches %>% filter(source_char %in% c("WFO", "196")) %>% pull(candidate_name) %>% unique()
    wfo_cands  <- unique(c(wfo_cands, wfo_included, wfo_syns))
    
    raw_summary <- tibble(
      input_species = sp,
      infraspecific_list_GRIN = if (length(grin_cands) == 0) NA_character_ else paste(sort(unique(grin_cands)), collapse = "; "),
      infraspecific_list_WFO = if (length(wfo_cands) == 0) NA_character_ else paste(sort(unique(wfo_cands)), collapse = "; "),
      infraspecific_list_WFO_included_variety = if(length(wfo_included) == 0) NA_character_ else paste(sort(wfo_included), collapse = "; "),
      infraspecific_list_WFO_synonyms = if(length(wfo_syns) == 0) NA_character_ else paste(sort(wfo_syns), collapse = "; "),
      infraspecific_list_OUTLINK_PAGE = if(length(page_candidates) == 0) NA_character_ else paste(sort(page_candidates), collapse = "; ")
    )
    raw_summary
  }, error = function(e) {
    message(sprintf("Error in parallel processing of '%s': %s", sp, e$message))
    return(tibble(input_species = sp,
                  infraspecific_list_GRIN = NA_character_,
                  infraspecific_list_WFO = NA_character_,
                  infraspecific_list_WFO_included_variety = NA_character_,
                  infraspecific_list_WFO_synonyms = NA_character_,
                  infraspecific_list_OUTLINK_PAGE = NA_character_))
  })
}

message(sprintf("Starting parallel stage for %d species with %d workers...", length(species_list), num_workers))
# choose a handler appropriate for your environment:
# handlers("txtprogressbar")    # console
# handlers("progress")          # RStudio / GUI
progressr::handlers("txtprogressbar")

# run with a progress bar
progressr::with_progress({
  p <- progressr::progressor(along = species_list)
  parallel_results <- future_map_dfr(
    species_list,
    function(sp) {
      p(message = paste0("Processing: ", sp))
      process_species_parallel(sp)
    },
    .options = furrr_options(seed = TRUE)
  )
})

message("Parallel stage complete.")

# ---- Stage B: consolidate and perform provenance checks in main process ----
clean_outlinks_in_df <- function(df) {
  if ("infraspecific_list_OUTLINK_PAGE" %in% names(df)) {
    df$infraspecific_list_OUTLINK_PAGE_clean <- df$infraspecific_list_OUTLINK_PAGE
  } else df$infraspecific_list_OUTLINK_PAGE_clean <- NA_character_
  
  df$infraspecific_list_combined <- apply(df, 1, function(row) {
    cells <- c(row['infraspecific_list_GRIN'], row['infraspecific_list_WFO'], row['infraspecific_list_OUTLINK_PAGE_clean'])
    combined <- unique(unlist(lapply(cells, function(cell) {
      if (is.na(cell) || !nzchar(cell)) return(character(0))
      unlist(str_split(as.character(cell), "\\s*;\\s*"))
    })))
    combined <- combined[!is.na(combined) & nzchar(combined)]
    if (length(combined) == 0) return(NA_character_)
    paste(sort(unique(combined)), collapse = "; ")
  })
  df
}

collect_candidates_for_reverify <- function(row) {
  sp <- row$input_species
  cells <- c(row$infraspecific_list_GRIN, row$infraspecific_list_WFO,
             row$infraspecific_list_WFO_included_variety, row$infraspecific_list_WFO_synonyms,
             row$infraspecific_list_OUTLINK_PAGE_clean, row$infraspecific_list_combined)
  parts <- unique(unlist(lapply(cells, function(cell) {
    if (is.na(cell) || !nzchar(cell)) return(character(0))
    unlist(str_split(as.character(cell), "\\s*;\\s*"))
  })))
  genus <- str_split(sp, "\\s+")[[1]][1]
  parts <- unique(parts[grepl(paste0("^", genus, "\\b"), parts, ignore.case = TRUE)])
  parts
}

parallel_results_clean <- clean_outlinks_in_df(parallel_results)
species_candidates <- parallel_results_clean %>%
  rowwise() %>%
  mutate(candidates = list(collect_candidates_for_reverify(cur_data_all()))) %>%
  select(input_species, candidates)

message("Starting provenance verification (serial in main process)...")
all_long_results_list <- list()
for (i in seq_len(nrow(species_candidates))) {
  sp <- species_candidates$input_species[i]
  cands <- species_candidates$candidates[[i]]
  if (length(cands) == 0) next
  long_rows <- list()
  for (cand in cands) {
    if (!infra_syntax_ok(cand, sp)) next
    Sys.sleep(query_delay)
    # try combined local lookup first to get rich results
    res <- query_taxa_resolver_local(cand, sources_ids = c(sources$WFO, sources$GRIN))
    parsed_any <- FALSE
    if (!is.null(res) && !is.null(res$names)) {
      parsed_any <- TRUE
      # iterate entries and convert into long rows
      for (ne in res$names) {
        br_list <- ne$results %||% list(ne)
        for (br in br_list) {
          match_type <- tolower(as.character(br$matchType %||% br$match_type %||% ""))
          if (match_type == "nomatch") next
          cand_name <- br$currentName %||% br$matchedName %||% ne$name %||% NA_character_
          if (is.na(cand_name) || !infra_syntax_ok(cand_name, sp) || is.na(canonicalize_infra(cand_name))) next
          prov <- determine_source(trim_s(cand_name))
          # fallback check if still OUTLINK
          if (prov$source == "OUTLINK") {
            fb <- fallback_check_candidate_sources(trim_s(cand_name))
            if (fb != "OUTLINK") {
              prov$source <- fb
              key_master <- paste0("prov|", trim_s(cand_name))
              lock <- flock::lock(prov_cache_lock)
              assign(key_master, prov, envir = prov_cache)
              saveRDS(as.list(prov_cache), prov_cache_file)
              flock::unlock(lock)
            }
          }
          matched <- NA_character_; status <- NA_character_
          if (!is.null(prov$wfo) && isTRUE(prov$wfo$ok)) { matched <- prov$wfo$matched; status <- prov$wfo$status }
          else if (!is.null(prov$grin) && isTRUE(prov$grin$ok)) { matched <- prov$grin$matched; status <- prov$grin$status }
          long_rows[[length(long_rows) + 1]] <- tibble(
            input_species = sp,
            infraspecific_name = trim_s(cand_name),
            source = prov$source,
            verifier_matched_name = na_if_null(matched),
            verifier_status = na_if_null(status)
          )
        }
      }
    }
    # If combined local lookup returned nothing, fall back to determine_source on the candidate itself
    if (!parsed_any) {
      prov <- determine_source(trim_s(cand))
      if (prov$source == "OUTLINK") {
        fb <- fallback_check_candidate_sources(trim_s(cand))
        if (fb != "OUTLINK") {
          prov$source <- fb
          key_master <- paste0("prov|", trim_s(cand))
          lock <- flock::lock(prov_cache_lock)
          assign(key_master, prov, envir = prov_cache)
          saveRDS(as.list(prov_cache), prov_cache_file)
          flock::unlock(lock)
        }
      }
      matched <- NA_character_; status <- NA_character_
      if (!is.null(prov$wfo) && isTRUE(prov$wfo$ok)) { matched <- prov$wfo$matched; status <- prov$wfo$status }
      else if (!is.null(prov$grin) && isTRUE(prov$grin$ok)) { matched <- prov$grin$matched; status <- prov$grin$status }
      long_rows[[length(long_rows) + 1]] <- tibble(
        input_species = sp,
        infraspecific_name = trim_s(cand),
        source = prov$source,
        verifier_matched_name = na_if_null(matched),
        verifier_status = na_if_null(status)
      )
    }
  }
  if (length(long_rows) > 0) all_long_results_list[[length(all_long_results_list) + 1]] <- bind_rows(long_rows)
}

all_long_results <- if (length(all_long_results_list) > 0) bind_rows(all_long_results_list) else tibble()
message("Provenance verification complete.")

# ---- final output ----
if (nrow(all_long_results) > 0) {
  long_slim <- all_long_results %>%
    mutate(source = ifelse(toupper(trimws(source)) == "BOTH", "WFO, GRIN", toupper(source))) %>%
    select(input_species, infraspecific_name, source) %>%
    distinct()
  final_excel <- "infraspecifics_verified.xlsx"
  writexl::write_xlsx(long_slim, final_excel)
  message(sprintf("Successfully wrote %d unique infraspecific records to '%s'", nrow(long_slim), final_excel))
} else {
  message("No infraspecific names were successfully verified.")
}

# ---- optional: helper to clear provenance cache (use for multiple runs) ----
clear_prov_cache <- function() {
  rm(list = ls(envir = prov_cache), envir = prov_cache)
  if (file.exists(prov_cache_file)) file.remove(prov_cache_file)
  message("Cleared prov_cache and on-disk cache file.")
}

##### end script ######
