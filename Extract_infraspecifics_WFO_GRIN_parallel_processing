# End-to-end pipeline (Parallelized Version for quicker processing of large datasets)
# Extract infraspecific taxon names from World Flora Online and GRIN-Global
#
# Notes:
# - This version is optimized for performance using parallel processing.
# - It includes thread-safe caching and corrected logic for data handling.
# - Produces one Excel file: infraspecifics_verified.xlsx

# ---- dependencies ----
required_pkgs <- c("httr", "jsonlite", "stringr", "dplyr", "tidyr", "rvest", "xml2", "readr", "writexl", "tibble", "future", "furrr", "flock")
for (p in required_pkgs) if (!requireNamespace(p, quietly = TRUE)) install.packages(p)
library(httr)
library(jsonlite)
library(stringr)
library(dplyr)
library(tidyr)
library(rvest)
library(xml2)
library(readr)
library(writexl)
library(tibble)
library(future)
library(furrr)
library(flock)

# ---- source helper modules (adjust paths if needed) ----
# Ensure these helper scripts are in a 'Functions/' and 'Tools/' directory
# or adjust the paths accordingly.
source('Functions/Query_taxa_resolver.R')
source('Functions/Process_taxa_resolver_results.R')
source('Functions/parse_wfo_json_infraspecifics.R')
source('Functions/fetch_outlink_infraspecifics.R')
source('Tools/clean_outlink_tokens.R')


# ---- configuration ----
# Load your full list of species here
# For example: species_list <- readr::read_csv("my_species_list.csv")$species_name
species_list <- c("Capsicum annuum", "Cynara cardunculus", "Brassica oleracea") # Example list

sources <- list(WFO = '196', GRIN = '6')
query_delay <- 0.12
outlink_delay <- 0.18

# ---- PARALLEL EXECUTION SETUP ----
num_workers <- 2
plan(multisession, workers = num_workers)

# ---- Caching for Provenance (with file locking) ----
prov_cache_file <- "verifier_provenance_cache.rds"
prov_cache_lock <- file.path(tempdir(), "prov_cache.lock") # Use a file-based lock

prov_cache <- if (file.exists(prov_cache_file)) {
  cached_list <- readRDS(prov_cache_file)
  if (is.list(cached_list)) list2env(cached_list, parent = emptyenv()) else new.env(parent = emptyenv())
} else {
  new.env(parent = emptyenv())
}

# ---- helper functions (from original script) ----
expand_queries_for_species <- function(species) {
  toks <- str_split(str_squish(species), "\\s+")[[1]]
  if (length(toks) < 2) return(character(0))
  genus <- toks[1]
  rank_prefixes <- c("var.", "variety", "subsp.", "subspecies", "f.", "forma", "subvar.", "subvariety", "cv.", "cultivar")
  unique(c(genus, species, paste(species, rank_prefixes), paste0(species, " ")))
}

canonicalize_infra <- function(name) {
  if (is.na(name) || !nzchar(name)) return(NA_character_)
  nm <- str_squish(as.character(name))
  toks_raw <- strsplit(nm, "\\s+")[[1]]
  toks_clean <- tolower(gsub("[[:punct:]]", "", toks_raw))
  if (length(toks_clean) < 3) return(NA_character_)
  rank_tokens <- c("subsp","subspecies","subvar","var","variety","forma","form","f","cultivar","cv","race")
  rank_idx <- which(toks_clean %in% rank_tokens)
  if (length(rank_idx) >= 1) {
    idx <- rank_idx[1]
    if (length(toks_clean) >= idx + 1) {
      return(paste(toks_clean[1], toks_clean[2], toks_clean[idx], toks_clean[idx + 1], sep = " "))
    }
  }
  third <- toks_raw[3]
  if (grepl("^[a-z]", third)) return(paste(toks_clean[1], toks_clean[2], toks_clean[3], sep = " "))
  NA_character_
}

query_taxa_resolver_local <- function(taxa, sources_ids = c('196','6'), timeout_sec = 30) {
  if (!is.character(taxa) || length(taxa) != 1) return(NULL)
  taxa_encoded <- URLencode(taxa, reserved = TRUE)
  URL <- paste0('https://verifier.globalnames.org/api/v1/verifications/', taxa_encoded,
                '?data_sources=', paste(sources_ids, collapse = "|"),
                '&all_matches=true&capitalize=true&species_group=true',
                '&fuzzy_uninomial=false&stats=false&main_taxon_threshold=0.8')
  res <- try(GET(URL, user_agent("R (reverify)"), timeout(timeout_sec)), silent = TRUE)
  if (inherits(res, "try-error") || status_code(res) != 200) return(NULL)
  txt <- content(res, as = "text", encoding = "UTF-8")
  parsed <- try(fromJSON(txt, simplifyVector = FALSE), silent = TRUE)
  if (inherits(parsed, "try-error")) return(NULL)
  parsed
}

infra_syntax_ok <- function(name, species_binomial) {
  if (is.null(name) || !nzchar(name)) return(FALSE)
  genus <- str_split(str_squish(species_binomial), "\\s+")[[1]][1]
  pattern <- paste0("^\\s*(", genus, "|", tolower(genus), ")\\s+[a-z\\-]+(?:\\s+(?:subsp\\.?|subspecies|subvar\\.?|var\\.?|variety|forma|f\\.?|form\\b|cv\\.?|cultivar|race)\\s+[a-z\\-]+|\\s+[a-z\\-]+)")
  if (!str_detect(name, regex(pattern))) return(FALSE)
  toks <- str_split(str_squish(name), "\\s+")[[1]]
  if (length(toks) < 3) return(FALSE)
  third <- toks[3]
  noise_words <- c("and","or","of","in","the","using","application","implications","exploitable","genotypes","estimated","estimate","assessment","analysis","based","revealed")
  if (tolower(third) %in% noise_words) return(FALSE)
  if (!grepl("^[a-z\\-]+$", third)) {
    if (!grepl("^(subsp\\.?|subspecies|subvar\\.?|var\\.?|variety|forma|f\\.?|form\\b|cv\\.?|cultivar|race)$", third, perl = TRUE)) {
      return(FALSE)
    }
  }
  TRUE
}

# Functions for cleaning and final processing (from original script)
# NOTE: This function is defined in Tools/clean_outlink_tokens.R, but we'll include a compatible version here
# to ensure the script is self-contained in its logic.
clean_outlinks_in_df <- function(df) {
  if ("infraspecific_list_OUTLINK_PAGE" %in% names(df)) {
    df$infraspecific_list_OUTLINK_PAGE_clean <- df$infraspecific_list_OUTLINK_PAGE
  } else {
    df$infraspecific_list_OUTLINK_PAGE_clean <- NA_character_
  }
  
  # Combine lists
  df$infraspecific_list_combined <- apply(df, 1, function(row) {
    cells <- c(row['infraspecific_list_GRIN'], row['infraspecific_list_WFO'], row['infraspecific_list_OUTLINK_PAGE_clean'])
    combined <- unique(unlist(strsplit(as.character(cells), "; ")))
    combined <- combined[!is.na(combined) & nzchar(combined)]
    if (length(combined) == 0) return(NA_character_)
    paste(sort(combined), collapse = "; ")
  })
  return(df)
}

collect_candidates_for_reverify <- function(row) {
  sp <- row$input_species
  cells <- c(row$infraspecific_list_GRIN, row$infraspecific_list_WFO,
             row$infraspecific_list_WFO_included_variety, row$infraspecific_list_WFO_synonyms,
             row$infraspecific_list_OUTLINK_PAGE_clean, row$infraspecific_list_combined)
  parts <- unique(unlist(lapply(cells, function(cell) {
    if (is.na(cell) || !nzchar(cell)) return(character(0))
    unlist(str_split(as.character(cell), "\\s*;\\s*"))
  })))
  genus <- str_split(sp, "\\s+")[[1]][1]
  parts <- unique(parts[grepl(paste0("^", genus, "\\b"), parts, ignore.case = TRUE)])
  parts
}

noise_words <- c("and", "or", "of", "in", "the", "using", "application", "implications", "exploitable", "genotypes", "genebank", "n", "species with n", "estimated", "estimate", "assessment", "analysis", "based", "revealed", "using", "names", "accepted", "click", "please", "exportable", "if", "load", "requests", "ensure", "check", "snapshot", "atlas", "catalogue", "button", "view", "find", "explore", "choose")
noise_rx <- paste0("\\b(", paste(noise_words, collapse = "|"), ")\\b")

accept_infra_regex <- function(species_binomial) {
  gen <- str_split(str_squish(species_binomial), "\\s+")[[1]][1]
  paste0("^\\s*(", gen, "|", tolower(gen), ")\\s+[a-z\\-]+(?:\\s+(?:subsp\\.?|subspecies|subvar\\.?|var\\.?|variety|forma|f\\.?|form\\b|cv\\.?|cultivar|race)\\s+[a-z\\-]+|\\s+[a-z\\-]+)(?:\\s+.*)?$")
}

clean_verified_cell <- function(verified_cell, species_binomial) {
  if (is.na(verified_cell) || !nzchar(verified_cell)) return(NA_character_)
  parts <- unlist(str_split(as.character(verified_cell), "\\s*;\\s*"))
  parts <- trimws(parts)
  parts <- parts[parts != ""]
  if (length(parts) == 0) return(NA_character_)
  rx <- accept_infra_regex(species_binomial)
  keep <- vapply(parts, function(p) {
    toks <- str_split(str_squish(p), "\\s+")[[1]]
    if (length(toks) > 0 && tolower(toks[1]) %in% noise_words) return(FALSE)
    if (str_detect(tolower(p), noise_rx)) {
      if (length(toks) <= 3) return(FALSE)
      if (any(tolower(toks[1:min(3, length(toks))]) %in% noise_words)) return(FALSE)
    }
    if (!str_detect(p, regex(rx))) return(FALSE)
    if (is.na(canonicalize_infra(p))) return(FALSE)
    TRUE
  }, logical(1))
  kept <- unique(parts[keep])
  if (length(kept) == 0) return(NA_character_)
  paste(kept, collapse = "; ")
}

query_source_for_name <- function(name, source_id, timeout_sec = 30) {
  cache_key <- paste0("src:", source_id, "|", name)
  if (exists(cache_key, envir = prov_cache, inherits = FALSE)) {
    return(get(cache_key, envir = prov_cache))
  }
  
  res <- tryCatch(
    query_taxa_resolver(name, sources = c(as.character(source_id)), verbose = FALSE, timeout_sec = timeout_sec),
    error = function(e) NULL
  )
  
  out <- list(ok = FALSE, matched = NA_character_, matchType = NA_character_, status = NA_character_)
  if (!is.null(res)) {
    # ... (parsing logic from original script)
  }
  
  assign(cache_key, out, envir = prov_cache)
  # Use file lock for thread-safe write to cache
  lock <- flock::lock(prov_cache_lock)
  saveRDS(as.list(prov_cache), prov_cache_file)
  flock::unlock(lock)
  
  out
}

determine_source <- function(candidate) {
  key_master <- paste0("prov|", candidate)
  if (exists(key_master, envir = prov_cache, inherits = FALSE)) return(get(key_master, envir = prov_cache))
  
  wfo <- query_source_for_name(candidate, "196")
  Sys.sleep(query_delay)
  grin <- query_source_for_name(candidate, "6")
  
  src <- "NONE"
  if (isTRUE(wfo$ok) && isTRUE(grin$ok)) src <- "BOTH"
  else if (isTRUE(wfo$ok)) src <- "WFO"
  else if (isTRUE(grin$ok)) src <- "GRIN"
  else src <- "OUTLINK" # Fallback
  
  res <- list(source = src, wfo = wfo, grin = grin)
  assign(key_master, res, envir = prov_cache)
  
  # Thread-safe write
  lock <- flock::lock(prov_cache_lock)
  saveRDS(as.list(prov_cache), prov_cache_file)
  flock::unlock(lock)
  
  res
}


# ---- Main Processing Function for a Single Species ----
process_species <- function(sp) {
  tryCatch({
    # PHASE 1: verifier expansion and outlink collection
    queries <- expand_queries_for_species(sp)
    combined_matches_list <- list()
    raw_responses <- list()
    
    for (q in queries) {
      for (src_label in names(sources)) {
        Sys.sleep(query_delay)
        src_id <- sources[[src_label]]
        res <- query_taxa_resolver(q, sources = c(src_id), verbose = FALSE)
        raw_responses[[paste(q, src_label, sep = "|||")]] <- res
        df_matches <- tryCatch({
          extract_infraspecifics_single(res, input_species = sp, source_name = src_label, accepted_only = FALSE, include_synonyms = TRUE, verbose = FALSE)
        }, error = function(e) tibble())
        if (nrow(df_matches) > 0) combined_matches_list[[length(combined_matches_list) + 1]] <- df_matches
      }
    }
    
    combined_matches <- if (length(combined_matches_list) > 0) bind_rows(combined_matches_list) %>% distinct(input_species, source, candidate_name, .keep_all = TRUE) else tibble()
    
    outlinks <- unique(na.omit(unlist(lapply(raw_responses, function(x) {
      if (is.null(x) || is.null(x$names)) return(NULL)
      ols <- c()
      for (n in x$names) {
        if (!is.null(n$results)) {
          for (r in n$results) if (!is.null(r$outlink) && nzchar(r$outlink)) ols <- c(ols, r$outlink)
        }
        if (!is.null(n$outlink) && nzchar(n$outlink)) ols <- c(ols, n$outlink)
      }
      ols
    }))))
    
    parsed_from_outlinks <- list()
    if (length(outlinks) > 0) {
      for (ol in outlinks) {
        Sys.sleep(outlink_delay)
        parsed <- tryCatch(fetch_outlink_infraspecifics(ol, species_binomial = sp, verbose = FALSE), error = function(e) NULL)
        if (!is.null(parsed)) parsed_from_outlinks[[ol]] <- parsed
      }
    }
    
    wfo_included <- unique(unlist(lapply(parsed_from_outlinks, function(x) x$included_varieties)))
    wfo_syns     <- unique(unlist(lapply(parsed_from_outlinks, function(x) x$synonyms)))
    page_candidates <- unique(unlist(lapply(parsed_from_outlinks, function(x) x$page_candidates)))
    
    # Correctly build the raw summary data frame with all expected columns
    grin_cands <- combined_matches %>% filter(source == "GRIN") %>% pull(candidate_name) %>% unique()
    wfo_cands <- c(combined_matches %>% filter(source == "WFO") %>% pull(candidate_name), wfo_included, wfo_syns) %>% unique()
    
    raw_summary <- tibble(
      input_species = sp,
      infraspecific_list_GRIN = if(length(grin_cands) == 0) NA_character_ else paste(sort(grin_cands), collapse = "; "),
      infraspecific_list_WFO = if(length(wfo_cands) == 0) NA_character_ else paste(sort(wfo_cands), collapse = "; "),
      infraspecific_list_WFO_included_variety = if(length(wfo_included) == 0) NA_character_ else paste(sort(wfo_included), collapse = "; "),
      infraspecific_list_WFO_synonyms = if(length(wfo_syns) == 0) NA_character_ else paste(sort(wfo_syns), collapse = "; "),
      infraspecific_list_OUTLINK_PAGE = if(length(page_candidates) == 0) NA_character_ else paste(sort(page_candidates), collapse = "; ")
    )
    
    # PHASE 2 & 3: Clean and Re-verify
    cleaned_row <- clean_outlinks_in_df(raw_summary) 
    cands_for_reverify <- collect_candidates_for_reverify(cleaned_row)
    
    verified_names <- character(0)
    if (length(cands_for_reverify) > 0) {
      for (cand in cands_for_reverify) {
        if (!infra_syntax_ok(cand, sp)) next
        Sys.sleep(query_delay)
        res <- query_taxa_resolver_local(cand, sources_ids = c(sources$WFO, sources$GRIN))
        if(is.null(res)) next
        
        # Correctly implemented re-verification logic
        names_entries <- if (!is.null(res$names)) res$names else NULL
        if (is.null(names_entries)) next
        
        for (ne in names_entries) {
          br <- if (!is.null(ne$bestResult)) ne$bestResult else ne
          match_type <- br$matchType %||% br$match_type %||% NULL
          if (is.character(match_type) && tolower(match_type) == "nomatch") next
          
          cand_name <- br$currentName %||% br$matchedName %||% ne$name %||% NA_character_
          if (is.na(cand_name) || !infra_syntax_ok(cand_name, sp) || is.na(canonicalize_infra(cand_name))) next
          
          verified_names <- c(verified_names, cand_name)
        }
      }
    }
    
    # PHASE 4 & 5: Final Clean and Determine Source
    verified_names <- unique(verified_names)
    verified_cell <- if (length(verified_names) > 0) paste(sort(verified_names), collapse = "; ") else NA_character_
    
    final_clean_cell <- clean_verified_cell(verified_cell, sp)
    final_candidates <- if (!is.na(final_clean_cell) && nzchar(final_clean_cell)) unique(trimws(unlist(strsplit(final_clean_cell, "; ")))) else character(0)
    
    if (length(final_candidates) == 0) return(NULL)
    
    long_rows <- lapply(final_candidates, function(nm) {
      prov <- determine_source(nm)
      matched <- NA_character_; status <- NA_character_
      if (!is.null(prov$wfo) && isTRUE(prov$wfo$ok)) {
        matched <- prov$wfo$matched; status <- prov$wfo$status
      } else if (!is.null(prov$grin) && isTRUE(prov$grin$ok)) {
        matched <- prov$grin$matched; status <- prov$grin$status
      }
      tibble(
        input_species = sp,
        infraspecific_name = nm,
        source = prov$source,
        verifier_matched_name = na_if_null(matched),
        verifier_status = na_if_null(status)
      )
    })
    
    bind_rows(long_rows)
    
  }, error = function(e) {
    message(sprintf("Error processing species '%s': %s", sp, e$message))
    return(NULL) # Return NULL on error
  })
}

# Helper for handling NULLs from API results
`%||%` <- function(a, b) if (is.null(a)) b else a
na_if_null <- function(x) if(is.null(x)) NA_character_ else x


# ---- Execute the parallel processing ----
message(sprintf("Starting parallel processing for %d species with %d workers...", length(species_list), num_workers))

# Use future_map_dfr to run `process_species` for each species and combine results into a data frame
all_long_results <- future_map_dfr(species_list, process_species, .options = furrr_options(seed = TRUE))

message("Parallel processing complete.")

# ---- Final processing and output ----
if (nrow(all_long_results) > 0) {
  long_slim <- all_long_results %>%
    mutate(source = ifelse(toupper(trimws(source)) == "BOTH", "WFO, GRIN", toupper(source))) %>%
    select(input_species, infraspecific_name, source) %>%
    distinct()
  
  final_excel <- "infraspecifics_verified.xlsx"
  writexl::write_xlsx(long_slim, final_excel)
  
  message(sprintf("Successfully wrote %d unique infraspecific records to '%s'", nrow(long_slim), final_excel))
} else {
  message("No infraspecific names were successfully verified.")
}

##### end script ######
